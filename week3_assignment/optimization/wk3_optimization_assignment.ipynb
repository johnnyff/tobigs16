{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 16기 3주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38719929, 0.90221489, 0.82153456])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i]*parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias          1.000000\n",
       "experience    1.185555\n",
       "salary        0.043974\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X,parameters)\n",
    "    p = 1/(1+np.exp(-z))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8165208408789865"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) = -(y_i log p_i + (1-y_i)log(1-p_i )) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X,parameters)\n",
    "    loss = -(y*np.log(p)+ (1-y)*np.log(1-p))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters)\n",
    "    loss = (y-y_hat) **2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X,y,parameters)\n",
    "    loss = (loss/n) #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3309819399598846"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)= \\Sigma(\\theta^{T}X_i - y_i)X_{ij}  $ \n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -\\Sigma(y_i-p_i)x_{ij} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X,parameters)\n",
    "        gradient = np.sum(y_hat-y)*X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = np.sum(p-y)*X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11170106084353978"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X,y,parameters,j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41.462432170395054, 22.59828681974506, 52.63880790393534]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "### 설명: \n",
    "batch_idx 함수는 N 길이의 X_train 데이터를 batch_size만큼씩 쪼개는 함수이다.\n",
    "batch_size는 나누어진 각각의 데이터셋의 크기를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= (learning_rate/n)\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38719911, 0.90221479, 0.82153433])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch:  \n",
    "- num_epoch:\n",
    "<br>\n",
    "\n",
    "BGD: 학습 한 번에 모든 데이터셋에 대해 기울기를 계산\n",
    "SGD: 학습 한 번에 임의의 데이터 하나에 대해서만 기울기 계산\n",
    "MGD: 학습 한 번에 데이터셋의 일부에 대해서만 기울기를 계산\n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
    "batch_size=1 -> SGD \n",
    "batch_size=k -> MGD  \n",
    "batch_size=whole -> BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch,parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, batch_size)\n",
    "            new_loss = batch_loss(X_batch,y_batch, parameters, loss_function, batch_size)\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.957465336283517  params: [0.73432952 0.95495944 0.22885644]  gradients: [0.03612474523757879, 0.00961293240328576, 0.02727796040981147]\n",
      "epoch: 100  loss: 0.43073039333096524  params: [-0.80431619  1.07943547 -0.98356582]  gradients: [0.004692735800553397, -0.005071347660981587, 0.005733398268262121]\n",
      "epoch: 200  loss: 0.3804563601599708  params: [-1.05587866  1.53826717 -1.45101827]  gradients: [0.0014202229141990622, -0.004019711897996503, 0.0039338260203632324]\n",
      "epoch: 300  loss: 0.35498144049018926  params: [-1.16414745  1.89072576 -1.79380916]  gradients: [0.0008744338623877406, -0.0030999215846952387, 0.00300158631922327]\n",
      "epoch: 400  loss: 0.33953954990213525  params: [-1.24115205  2.16750935 -2.06101677]  gradients: [0.0006893113406021239, -0.002480882704743082, 0.002388005636498153]\n",
      "epoch: 500  loss: 0.3293947625574559  params: [-1.30420182  2.39246026 -2.27700365]  gradients: [0.0005793690248292514, -0.002046141688858768, 0.001959729399045266]\n",
      "epoch: 600  loss: 0.3223558900263979  params: [-1.35784925  2.58014253 -2.45637589]  gradients: [0.0004978008218971633, -0.0017258361666580581, 0.0016459567786527973]\n",
      "epoch: 700  loss: 0.3172737225838438  params: [-1.40424322  2.73982692 -2.60839118]  gradients: [0.00043303250249749367, -0.0014805036826926838, 0.001406874765518688]\n",
      "epoch: 800  loss: 0.3134918445362202  params: [-1.44479339  2.87773481 -2.739234  ]  gradients: [0.00038021182469023275, -0.0012867799092614173, 0.0012189724289983158]\n",
      "epoch: 900  loss: 0.31061032189638876  params: [-1.48053743  2.99823624 -2.85322892]  gradients: [0.0003364231380549792, -0.0011300723111726395, 0.001067617452342912]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.51197119,  3.10351754, -2.95257309])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train,learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = len(X_train))\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.4448827511170929  params: [0.55861413 0.52348427 0.04701062]  gradients: [0.021563596868057852, 0.019474746089627182, 0.024705281082810948]\n",
      "epoch: 100  loss: 0.08192717948630324  params: [-1.49224955  3.0232707  -2.88460016]  gradients: [0.003349928275731274, 0.0047612380754173626, 0.006100319939976631]\n",
      "epoch: 200  loss: 0.07070198012604512  params: [-1.68890794  3.66984848 -3.49027612]  gradients: [0.0031831289135861515, 0.004657797882577589, 0.0055755586988680056]\n",
      "epoch: 300  loss: 0.06671852899359812  params: [-1.77577882  3.95165296 -3.7517048 ]  gradients: [0.003140636954836517, 0.004628853157290484, 0.005393852255619471]\n",
      "epoch: 400  loss: 0.0649068436463207  params: [-1.81933458  4.0923591  -3.88168784]  gradients: [0.0031246582315224676, 0.004617431974754146, 0.0053122511230640525]\n",
      "epoch: 500  loss: 0.06399535160162786  params: [-1.84235488  4.16658872 -3.95011733]  gradients: [0.003117449862270901, 0.004612137557396955, 0.005271468489272969]\n",
      "epoch: 600  loss: 0.06351393048735139  params: [-1.85483563  4.20679612 -3.98714241]  gradients: [0.003113872028080457, 0.00460946866226245, 0.005250005707361841]\n",
      "epoch: 700  loss: 0.06325318870882374  params: [-1.86169208  4.22887375 -4.00746062]  gradients: [0.003112001164383817, 0.004608060772795657, 0.005238404172219006]\n",
      "epoch: 800  loss: 0.06311005452783056  params: [-1.86548547  4.24108517 -4.01869524]  gradients: [0.003110994260096893, 0.00460729928127719, 0.005232042443954706]\n",
      "epoch: 900  loss: 0.06303090150255047  params: [-1.86759231  4.24786639 -4.0249329 ]  gradients: [0.003110443581757069, 0.004606881660435585, 0.005228526542532598]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.86875629,  4.25161253, -4.02837842])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.27016067577460734  params: [-0.898466    1.17511657 -1.29571397]  gradients: [0.02447561621024984, 0.0133196537517638, 0.01722081696512671]\n",
      "epoch: 100  loss: 0.07736667399201351  params: [-1.9303261   4.17502072 -4.06769225]  gradients: [0.007538539554837429, 0.004102480436115042, 0.005304046637398144]\n",
      "epoch: 200  loss: 0.07736266551822696  params: [-1.9303681   4.1751431  -4.06780374]  gradients: [0.0075381595214906935, 0.004102273621604259, 0.00530377924945381]\n",
      "epoch: 300  loss: 0.07736266518361704  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489767116, 0.0041022736043402576, 0.0053037792271333935]\n",
      "epoch: 400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.9303681 ,  4.17514311, -4.06780375])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 1)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 4,  6]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "print(\"accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6127785 , 2.64798101])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.8156953966762852  params: [1.03793914 1.04308147]  gradients: [-0.022480444128780766, -0.02482580022663864]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.66047422, 2.57778457])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y,learning_rate = 0.1, num_epoch = 50, tolerance = 0.00001, model = 'linear', batch_size = 16 )\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> epoch이 커질수록 파란선이 빨간선에 다가가는 것을 확인 할 수 있다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
